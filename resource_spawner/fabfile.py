import os
import re
import copy
import tempfile
import time
import datetime
import subprocess
import paramiko
import commands
import json
import pycurl
import cStringIO
import yaml


from fabric.api import env,parallel, roles, run, settings, sudo, execute,hide,task
from fabric.operations import run, put,sudo
from fabric.state import output
from fabric.exceptions import CommandTimeout
from time import sleep

paramiko.util.log_to_file("/dev/null")

### If upgrade target is not specified take the default value "target" for target snapshot
try: env.upgrade
except Exception:  env.upgrade =  'target'

"""
fabConfig: read fabric.yaml and configure fab.
  It also setup user.yaml in hiera
Arguments:
  config_yaml: location of fabric.yaml
  user_yaml: if set, the fab node configuration will be get from user_yaml 
             instead of getting from undercloud nova.
"""

@task
def fabConfig(config_yaml,user_yaml=None,snapshot=0):
  """ read fabric.yaml and configure fab. """
  global fabric_config
  ## Read the yaml file and set global variables
  
  log("Configuring Fabric")
  fabric_config=rdYaml(config_yaml)
  base_dir=os.path.basename(config_yaml)
  ## Get node details dict
  ## If user_yaml provided, get node_config from that file, or generate by contacting undercloud nova
  if not user_yaml:
    log('Generating node configuration from undercloud nova')
    node_config = createResourceYaml(fabric_config['fabric::auth_user'],fabric_config['fabric::auth_password'],fabric_config['fabric::auth_tenant'],fabric_config['fabric::auth_url'],fabric_config['project'])
  else:
    log("Extracting node configuration from user.yaml")
    useryaml_dict =  rdYaml(user_yaml)
    node_config = useryaml_dict['nodes']
 
  ##Add apt_snapshot_version in user.yaml
  if snapshot != 0:
    if not fabric_config.has_key('apt_snapshot_version'):
      fabric_config.update({'apt_snapshot_version': snapshot})
    else:
      fabric_config['apt_snapshot_version'] = snapshot
  
  nodes = { 'all': []}
  for node_type in node_config:
    if not nodes.has_key(node_type):
      nodes.update({node_type: []}) 
    if node_type != 'all':
      nodes['all'].extend(node_config[node_type].values()) 
      nodes[node_type] = node_config[node_type].values()
  
  ## Setup env.roldefs
  env.roledefs = nodes
  
  if not user_yaml:
    ## set ssh key file 
    env.key_filename = fabric_config['fabric::ssh_key']
  
    ## Set env.hosts
    env.hosts = fabric_config['fabric::floating_ip']
 
  ### Set ssh user
  env.user = 'ubuntu'

  ### Call genUserHiera() to generate user.yaml for hiera overrides
  if not user_yaml:
    user_yaml=fabric_config['fabric::directory'] + '/' + fabric_config['fabric::user_yaml']
  genUserHiera(fabric_config,node_config,user_yaml)

"""
genUserHiera - to generate user.yaml from fabric.yaml and node config

Parameters:
  fabconfig_dict: dict object generated from fabric.yaml
  nodeconfig_dict: dict object generated by query undercloud nova.
  user_yaml: user.yaml file path
"""
def genUserHiera(fabconfig_dict,nodeconfig_dict,user_yaml): 
  ### add nodes dict to  useryaml 
  useryaml_config = {'nodes': nodeconfig_dict}

  ### Add all non-fabric config nodes to user yaml dict
  for key,item in fabconfig_dict.items():
    if not re.match("fabric::", key):      
      n = {key:item}
      useryaml_config.update({key:item})

  ## Write to yaml file
  wtYaml(useryaml_config,user_yaml,False)

## rdYaml: Read yaml from a file and return a dict object
## Arguments:
### file: file path to read
## Return: python dict object

def rdYaml(file_path):
  fs = file(file_path,'r')
  output =  yaml.load(fs)
  return output

## wtYaml: take a python object and write to provided file.
## Arguments:
## data: python dictionary object
## file_path: file path
## append: True or False whether to append existing the yaml file or not

def wtYaml(data,file_path,append=False):
  if append == False:
    fs = file(file_path,'w')
  else:
    fs = file(file_path,'a')

  yaml.dump(data,fs)


## initiateSetup: initiate the new setup from caller machines usually this will be jenkins server
### This will call fab from lb/jumphost server

@task
def initiateSetup():
  verbose = fabric_config['fabric::verbose']
  ### Print stdout and error messages only on verbose 
  if verbose == False:
    output.update({'running': False, 'stdout': False, 'stderr': False})

  log("Copying the files to lb node")
  dir =  fabric_config['fabric::directory']
  base_dir=os.path.basename(dir)
  tarfile='%s-%s.tar.gz' % (base_dir,int(time.time()))
  os.system('cp fabfile.py %s/jiocloud_puppet_builder/resource_spawner; cd %s; tar zcf %s %s' % (dir,os.path.dirname(dir),tarfile,base_dir))
  ## Copy files to lb
  execute(putFiles,hosts=env.hosts,files = {os.path.dirname(dir) + '/' + tarfile: '/tmp'})

  ### Untar and copy fab file and ssh private key and puppet code to lb
  run('cd /tmp; tar  -zxf %s; cp -r /tmp/%s/jiocloud_puppet_builder/resource_spawner/{fabfile.py,fabric.yaml} ~/; cp -f /tmp/%s/id_rsa ~/.ssh' % (tarfile,base_dir,base_dir))
  sudo ('cp -r /tmp/%s/jiocloud_puppet_builder /var/puppet' % base_dir) 

  log("Setting up the system on %s" % env.host)
  log("Run userdata.sh on lb node")
  sudo("bash /tmp/%s/jiocloud_puppet_builder/resource_spawner/userdata.sh  -r lb" % base_dir)

  log("Run fab from lb1 to setup the cloud: %s" % fabric_config['project'])

  ## Enable output - it is required to print inner fab messages
  output.update({'running': True, 'stdout': True, 'stderr': True})
  
  ## If base_Version is not 0, this is an upgrade test
  ## so upgrade the system to base version, setup cloud
  ## Verify the setup,
  ## Then upgrade the system to target version, then verify.

  if fabric_config['base_version'] != 0:
    log("Fab run to setup cloud with base snapshot version: %s" % fabric_config['base_version'] )
    inner_fabrun_base_version=run('fab -f ~/fabfile  -i ~ubuntu/.ssh/id_rsa fabConfig:/tmp/%s/jiocloud_puppet_builder/resource_spawner/fabric.yaml,user_yaml=/tmp/%s/%s,snapshot=%s setup'  % (base_dir,base_dir,fabric_config['fabric::user_yaml'],fabric_config['base_version']))
    if inner_fabrun_base_version.return_code == 0:
      log("Fab run to setup cloud with base version: SUCCESS")
    else:
      log("Fab run to setup cloud with base version: FAILED")
      return 1
    
    log("Fab run to setup cloud with target snapshot version: %s" % fabric_config['target_version'])
    inner_fabrun_target_version=run('fab -f ~/fabfile  -i ~ubuntu/.ssh/id_rsa fabConfig:/tmp/%s/jiocloud_puppet_builder/resource_spawner/fabric.yaml,user_yaml=/tmp/%s/%s,snapshot=%s setup'  % (base_dir,base_dir,fabric_config['fabric::user_yaml'],fabric_config['target_version']))
    if inner_fabrun_target_version.return_code == 0:
      log("Fab run to setup cloud with target version: SUCCESS")
    else:
      log("Fab run to setup cloud with target version: FAILED")
      return 1

  ## if base_version is zero, this is functional test
  ## so upgrade to target version, setup cloud and verify
  else:
    log("Fab run to setup cloud with target snapshot version: %s" % fabric_config['target_version'])
    inner_fabrun_target_version=run('fab -f ~/fabfile  -i ~ubuntu/.ssh/id_rsa fabConfig:/tmp/%s/jiocloud_puppet_builder/resource_spawner/fabric.yaml,user_yaml=/tmp/%s/%s,snapshot=%s setup'  % (base_dir,base_dir,fabric_config['fabric::user_yaml'],fabric_config['target_version']))
    if inner_fabrun_target_version.return_code == 0:
      log("Fab run to setup cloud with target version: SUCCESS")
    else:
      log("Fab run to setup cloud with target version: FAILED")
      return 1

  os.system('rm -fr %s' % dir)


##Python logger doesnt work with fab
def log(msg):
  print str(datetime.datetime.now()) + ' ' + msg

##This Function to get the token from keystone 
##This Function will return the keystone response in json 
def getToken(username, password, tenantname, keystoneurl):
    keystoneurl = keystoneurl+"/tokens"
    postData = '{"auth": {"tenantName": "%s",\
        "passwordCredentials":{"username":"%s", "password": "%s"}\
        }}' % (tenantname, username, password)
    response = cStringIO.StringIO()
    try:
        c = pycurl.Curl()
        c.setopt(pycurl.URL, keystoneurl)
        c.setopt(pycurl.HTTPHEADER, ['Content-Type:application/json'])
        c.setopt(pycurl.POST, 1)
        c.setopt(pycurl.POSTFIELDS, postData)
        c.setopt(c.WRITEFUNCTION, response.write)
        c.perform()
        status_code = c.getinfo(pycurl.HTTP_CODE)
        if status_code == 200:
            return response
        else:
            log(response.getvalue())
            response.close()
            response = cStringIO.StringIO()
            response.write("NULL")
            return response
    except Exception, err:
        print Exception, err
        response.close()
        response = cStringIO.StringIO()
        response.write("NULL")
        return response

##This function will return vm list in json format
def getVmList(token, nova_endpoint):
    response = cStringIO.StringIO()
    xauth = str("X-Auth-Token: %s" % token)
    try:
        c = pycurl.Curl()
        c.setopt(pycurl.URL, nova_endpoint)
        c.setopt(pycurl.HTTPHEADER, [xauth])
        c.setopt(c.WRITEFUNCTION, response.write)
        c.perform()
        status_code = c.getinfo(pycurl.HTTP_CODE)
        if status_code == 200:
            return response
        else:
            log(response.getvalue())
            response.close()
            response = cStringIO.StringIO()
            response.write("NULL")
            return response
    except Exception, err:
        print Exception, err
        response.close()
        response = cStringIO.StringIO()
        response.write("NULL")
        return response


##This Function will create a yaml file which will be having hostname and ipaddress 
def createResourceYaml(username, password, tenant_name, auth_url, project_name, file_path=None,append=True):
    nodes = {}
    DataFound = getToken(username, password, tenant_name, auth_url)
    if DataFound.getvalue() != "NULL":
        datajson = json.loads(DataFound.getvalue())
        token = datajson['access']['token']['id']
        nova_endpoint = str(datajson['access']['serviceCatalog']
                            [0]['endpoints'][0]['publicURL']+"/servers/detail")
        DataNova = getVmList(token, nova_endpoint)
        if DataNova.getvalue() != "NULL":
            data_nova = json.loads(DataNova.getvalue())
            for vm in data_nova['servers']:
                name_vm = vm['name']
                split_name = name_vm.split("_")
                storage_access = "stg_access_%s" % project_name
                try:
                    if split_name[1] == project_name:
                        vm_meta = vm['metadata']['host_type']
                        if nodes.has_key(vm_meta):
                            nodes[vm_meta].update({str(split_name[0]):
                                          str(vm['addresses']
                                          [storage_access][0]['addr'])})
                        else:
                            nodes.update({str(vm_meta):{str(split_name[0]):
                                                   str(vm['addresses']
                                                   [storage_access][0]['addr'])}})
                except:
                    log("Variable Not Initialized")
            if file_path:
              wtYaml(nodes, file_path, append)
            return nodes
        else:
            log("Nova api call failed")
            return 100
    else:
        log("Keystone Api call failed")
        return 100


## This expect a dictionary of files with source -> destination format 
## will files from source directory to destination

@parallel
def putFiles(files,run_type=run):
#  with hide('warnings'), settings(warn_only = True):
    for key in files:
      print "%s -> %s" % (key,files[key])
      ## Make parent directory
      run_type('mkdir -p %s' % files[key])
      if run_type == sudo:
        put (key,files[key],use_sudo=True)
      else:
        put(key,files[key])

## run commands parallel on specified machines
### command - an arbiterary command
### run_type - two values - run, sudo, to determine whether to run as current user or root

@parallel
def runCommand(cmd,run_type):
  if run_type == 'sudo':
    sudo(cmd)
  elif run_type == 'run':
    run(cmd)
  else:
    log("No valid runtype %s" % run_type)


## Setup the entire system 
### Copy userdata file  to /tmp of all systems - roldefs['all']
### Run userdata file to do basic setup and install puppet
### Run puppet on the systems to setup openstack and other applications

@task
def setup(verify=True):
  """Setup the entire cloud system""" 
  timeout = 5
  maxAttempts = 40
  verbose = fabric_config['fabric::verbose']
  ### Print stdout and error messages only on verbose 
  if verbose == False:
    output.update({'running': False, 'stdout': False, 'stderr': False})
  log("Waiting till all servers are sshable")
  while not verifySshd(env.roledefs['all'],'ubuntu'):
      sleep(5)
      continue
  log("all nodes are sshable now")
  log("Copy data to All except lb servers")
  nodes_to_run_userdata =  env.roledefs['oc'] + env.roledefs['cp'] + env.roledefs['cp'] + env.roledefs['st'] + env.roledefs['db']
  dir =  '/tmp/' + fabric_config['project']
  ## Make tar
  log("Copy changed user.yaml to local hiera directory")
  execute(putFiles,hosts='localhost',files = {dir + '/' + fabric_config['fabric::user_yaml']: '/var/puppet/hiera/'},run_type='sudo')

  log("Make a tar of all files and copy it to all cloud servers")
  os.system('cd %s/..; tar zcf  %s.tar.gz %s' % (dir,dir,os.path.basename(dir)))
  ## Copy the tar
  execute(putFiles,hosts=nodes_to_run_userdata,files = {dir + ".tar.gz": '/tmp'})

  log("Running userdata script on all servers")
  ## Untar, copy puppet files, run userdata
  command_to_run='tar -C %s -zxf %s.tar.gz; cp -r %s/jiocloud_puppet_builder  /var/puppet/; bash %s/jiocloud_puppet_builder/resource_spawner/userdata.sh -r non-lb -p http://%s:3128' % (os.path.dirname(dir),dir,dir,dir,env.roledefs['lb'][0])

  execute(runCommand,hosts=nodes_to_run_userdata,cmd=command_to_run,run_type="sudo")

  status = 1
  attempt = 1
  ## Configure contrail vm - this is only applicable for 
  ## vm spawned from contrail golden image
  log("Configuring contrail node")
  execute(configContrail,hosts='10.1.0.245')
  ## Run puppet - first run on all servers
  log("Initial execution of puppet run on storage")
  with hide('warnings'), settings(warn_only = True):
    execute(runPapply,hosts=env.roledefs['all'])
  
  ## Reduce the wait time for cloud-init-nonet on cp nodes as they have vhost0 which would not be coming up by that.
    execute(reduceCloudinitWaitTime,hosts=env.roledefs['cp'])
  
## Run papply on LB and DB nodes
  log("Running papply on LB and db nodes")
  with hide('warnings'), settings(warn_only = True):
    execute(runPapply,hosts=['10.1.0.5','10.1.0.10','10.1.0.53','10.1.0.52','10.1.0.51'] ) 

  ## Sync the time initially to avoid ntp to take longer to sync te clocks.
    log("Executing ntpdate for initial time sync")
    try: 
      execute(runNtpdate,hosts='10.1.0.5')
    except Exception:
      log('Failed time sync on lb, retrying')
      execute(runNtpdate,hosts='10.1.0.5')
    try:
      execute(runNtpdate,hosts=env.roledefs['all'])
    except Exception:
      log('Failed time sync on lb, retrying')
      execute(runNtpdate,hosts=env.roledefs['all'])
  ##
  log("Checking ceph mon status")
  with hide('warnings'), settings(host_string = '10.1.0.53', warn_only = True):
    st_ceph_mon = sudo ("ceph mon stat | grep st1,st2,st3")
    if st_ceph_mon.return_code != 0:
      log("Ceph Mons are not up, fixing")
      while st_ceph_mon.return_code != 0:
        execute(runPapply,hosts=['10.1.0.53','10.1.0.52','10.1.0.51'])
        with  settings(host_string = '10.1.0.53', warn_only = True):
          execute(waitForSSH)
          try:
	    st_ceph_mon = sudo ("ceph mon stat | grep st1,st2,st3")
          except Exception:
            log("Failed checking mon stat, retrying")
	    st_ceph_mon = sudo ("ceph mon stat | grep st1,st2,st3")

  log("ceph mon are up, running papply on db, cp and oc nodes")
  nodes = ['10.1.0.10'] + env.roledefs['cp'] + env.roledefs['oc'] + env.roledefs['st']
  execute(runPapply,hosts=nodes)

  log("Restarting ntp on all servers")
  execute(restartNtp,hosts=env.roledefs['all'])  

  log("Configuring All CP nodes")
  execute(configCP,hosts=env.roledefs['cp'])
  execute(runPapply,hosts=nodes)
  if verify == True:
    execute(checkAll,verbose=verbose)

### Verify the services are up and running.
def checkAll(verbose='no'):
  """check all components and return on success, optional argument, 'verbose' for verbose output""" 

  if verbose.upper() == 'NO':
    output.update({'running': False, 'stdout': False, 'stderr': False})

#  rv = {'checkCephOsd': 1, 'checkCephStatus': 1, 'checkNova': 1, 'checkCinder': 1, 'getOSControllerProcs': 1 }
  rv = {'checkCephOsd': 1, 'checkCephStatus': 1, 'checkNova': 1, 'checkCinder': 1, }
  status = 1
  timeout = 5
  initial_prun = 1
  maxAttempts = 40
  attempt = 1
  log("Start checking the cluster")
  log("Checking the services")
  success = 0
  while ( attempt < maxAttempts ):
    attempt += 1
    time.sleep(timeout)
    if status != 0:
      log("System is not up... checking....")
      status = 0 
      for key,val in rv.items():
        if val != 0:
          log( "Executing %s" % key)
          rv[key] = execute(key).values()[0]
          if rv[key] != 0:
            status = 1
    else:
      attempt = maxAttempts        
      success = 1

  if success == 1:
    log("The openstack cloud is up and running")
    return 0
  else:
    log("Something failed, exiting")
    return 100


@parallel
def reduceCloudinitWaitTime():
    sudo('sed -i "s/long=[0-9]*/long=10/g" /etc/init/cloud-init-nonet.conf')

def waitForSSH():
  while not verifySshd([env.host],'ubuntu'):
    sleep(5)
    continue

##Check nova status
def checkNova():
  """Check Nova services are enabled"""
  nodes=env.roledefs['oc'] + env.roledefs['cp']
  dict_hosts=getHostname(nodes)
  failed_hosts=[]
  for key,val in dict_hosts.items():
    with hide('warnings'), settings(host_string = '10.1.0.11', warn_only = True):
      novamanage_status = sudo('nova-manage service list 2> /dev/null | grep "%s.*enabled.*:-)"' % val)
      if novamanage_status.return_code != 0:
	log("Nova is not up for %s" % val)
        failed_hosts.append(key)
  if len(failed_hosts) != 0:
    execute(runPapply,hosts = failed_hosts)
  else:
    log("Nova is running on all nodes")
  return len(failed_hosts)

## Check cinder
def checkCinder():
  """Check Cinder services are enabled"""
  nodes=env.roledefs['oc'] 
  dict_hosts=getHostname(nodes)
  failed_hosts=[]
  for key,val in dict_hosts.items():
    with hide('warnings'), settings(host_string = '10.1.0.11', warn_only = True):
      novamanage_status = sudo('cinder-manage service list 2> /dev/null | grep "%s.*enabled.*:-)"' % val)
      if novamanage_status.return_code != 0:
        log("Cinder services are not up for %s" % val)
        failed_hosts.append(key)
  if len(failed_hosts) != 0:
    execute(runPapply,hosts = failed_hosts)
  else:
    log("Cinder services are up and running on all nodes")
  return len(failed_hosts)


def configContrail():
  """Configure contrail"""
  with hide('warnings'), settings( warn_only = True):
    sudo("sed -i s/pp11.jiocloud.com/__project__.jiocloud.com/g /etc/contrail/svc_monitor.conf /etc/contrail/config.global.js /etc/contrail/openstackrc /etc/contrail/api_server.conf /etc/neutron/neutron.conf")
    execute(rebootNode,hosts=env.host)

@parallel
def checkInitialPapplyRun():
  """Check initial puppet apply has been completed"""
  maxAttempts=30
  attempt = 1
  with hide('warnings'), settings(warn_only = True):
    while ( attempt < maxAttempts ):
      attempt += 1
      time.sleep(5)
      initial_papply_run = run('ps -efw | grep /var/puppet/bin/papply || test -e /var/log/papply.log')
      if initial_papply_run.return_code == 0:
        log(env.host + " Initial Puppet run completed")
        return 0
  return initial_papply_run.return_code

@parallel
def restartNtp():
  """Restarts ntp server"""
  sudo("service ntp restart")

@parallel
def runNtpdate():
  """Runs ntpdate which is required for initial time sync"""
  with hide('warnings'), settings(warn_only = True):
    if env.host_string == '10.1.0.5':
       sudo("ntpdate -u 10.135.121.138 10.135.121.107")
    else:
      sudo("ntpdate -u 10.1.0.5")
  
def rebootNode():
  with hide('warnings'), settings(warn_only = True):
    try:
      sudo("reboot --force", timeout=5)
    except CommandTimeout:
      pass
    log('Reboot issued; Waiting for the node (%s) to go down...' % env.host)
    wait_until_host_down(wait=300, host=env.host)
    log ('Node (%s) is down... Waiting for node to come back' %   env.host)
    while not verifySshd([env.host],'ubuntu'):
      sleep(5)
      continue
    log("All nodes are sshable now")

@parallel
def runPapply(num_exec=1):
  """Run puppet apply"""
  with hide('warnings'), settings(warn_only = True):
    attempt=1 
    while ( attempt <= num_exec ):
      log(env.host + ' Running Puppet (num_exec: %s)' % attempt)
      if env.upgrade == 'base':
        try:
          papply_out = sudo('/var/puppet/bin/papply -b')
        except Exception:
          log("Failed runPapply, retrying")
          papply_out = sudo('/var/puppet/bin/papply -b')
      else:
        try:
          papply_out = sudo('/var/puppet/bin/papply')
        except Exception:
          log("Failed runPapply, retrying")
          papply_out = sudo('/var/puppet/bin/papply')

      if re.search('dpkg was interrupted, you must manually run \'sudo dpkg --configure -a\' to correct the problem',papply_out):
        sudo('dpkg --configure -a')
      execute(rebootIfNeeded,hosts=env.host)
      attempt += 1
      while not verifySshd([env.host],'ubuntu'):
        sleep(5)
        continue
  log("Done puppet apply on %s" % env.host)


@parallel(pool_size=5)
def configCP():
  """Configure CP server"""
  with hide('warnings'), settings(warn_only = True):
    log("Rebooting Node forcefully")
    execute(rebootNode,hosts=env.host)
    execute(runPapply,hosts=env.host)

### Rebooting from puppet is causing fab to hang, so disabled autoreboot in puppet and handling the reboot in fab
def rebootIfNeeded():
  """Reboot the node if required"""
  if env.host != '10.1.0.5':
   with hide('warnings'), settings(warn_only = True):
    rv_needreboot = sudo ('grep System.restart.required /var/run/reboot-required')
    if rv_needreboot.return_code == 0:
      execute(rebootNode,hosts=env.host)

def restartCephMon():
  """Restart ceph mon"""
  with hide('warnings'), settings(warn_only = True):
    log(env.host + " Restarting ceph mon")
    sudo("/etc/init.d/ceph restart mon")

def checkSvnUpdated():
  """check svn checkout is done"""
  for node in env.roledefs['all']:
    with hide('warnings'), settings(host_string = node, warn_only = True):
#      with hide('running', 'stdout', 'stderr'):
        st_svnup = sudo('svn up --username svn --password SubVer510n@1234 --trust-server-cert --non-interactive /var/puppet/')
        if st_svnup.return_code == 0:
          log(node + " svn update is working")
        else:
          sudo('svn cleanup; echo p | svn -q up --username svn --password SubVer510n@1234 /var/puppet &> /dev/null') 
          log (node + "svn update Failed")
          return 1
  return 0
  

def checkAptUpdated():
  """check apt get update status"""
  for node in env.roledefs['all']:
    with hide('warnings'), settings(host_string = node, warn_only = True):
#      with hide('running', 'stdout', 'stderr'):
        st_aptup = sudo('apt-get  update')
        if st_aptup.return_code == 0:
          log( node + " apt-get update done")
        else:
          log(node + " apt-get update failed") 
          return 1
          execute(runPapply,hosts=node)
  return 0

def cephMonStatus():
  hostlist=[env.roledefs['st'][0]]
  for node in hostlist:
    with hide('warnings'), settings(host_string = node, warn_only = True):
      st_ceph_mon = sudo ("ceph mon stat | grep st1,st2,st3")
      if st_ceph_mon.return_code == 0:
        log("Ceph Mons are running")
        return 0
      else:
        return 1

def checkCephStatus():
  """run ceph status on specific node"""
  hostlist=[env.roledefs['st'][0]]
  for node in hostlist:
    with hide('warnings'), settings(host_string = node, warn_only = True):
      st_ceph = sudo ("ceph -s | grep 'health *HEALTH_OK\|health *HEALTH_WARN clock skew detected'")
      if st_ceph.return_code == 0:
         log("ceph is healthy")
         return 0
      else:
         log("Ceph failed ") 
         execute(runPapply,hosts=env.roledefs['st'])
         execute(restartCephMon,hosts=env.roledefs['st'])
         return 1
  return 0
  
def checkCephOsd():
  """check minimum one OSD is in and up"""
  with hide('warnings'), settings(host_string = env.roledefs['st'][0], warn_only = True):
    osdUp = sudo ('ceph osd  dump  | grep "up *in"')
    osdUpList = osdUp.split('\n')
    minOsdUpPerNode = 0
    for node in env.roledefs['st']:
      osdUpInNode = 0
      for osd in osdUpList:
        if re.search(node,osd):
	  osdUpInNode = 1
      if osdUpInNode == 0:
        log("Node %s, yet to have minimum OSDs up" % node)
        minOsdUpPerNode = 1
        execute(runPapply,hosts=node)
    return minOsdUpPerNode

def wait_until_host_down(wait=120, host=None):
  if not host:
    host = env.host
  timeout = 5
  attempts = int(round(wait / float(timeout)))
  i = 0
  while i < attempts:
    res = subprocess.call(['ping', '-c', '1', host],
        stdout=subprocess.PIPE,
	stderr=subprocess.PIPE)
    if res is not 0:
      return
    time.sleep(timeout)
    i += 1
  print 'Timeout while waiting for host to shut down.'
  sys.exit(1)


def verifySshd(hostlist=[env.host], user='ubuntu',pkey_file='/home/ubuntu/.ssh/id_rsa'):
  rv_ssh = True
  for node in hostlist:
    log("Trying %s" % node)
    try:
      private_key = paramiko.RSAKey.from_private_key_file(pkey_file)
      client = paramiko.SSHClient()
      client.set_missing_host_key_policy(
        paramiko.AutoAddPolicy())
      client.connect(node,username=user,password='',pkey=private_key)
      client.close()
    except Exception:
      log("Waiting for %s to be sshable" % node)
      rv_ssh = False
  return rv_ssh


#@parallel
def getOSControllerProcs():
  procs_down = []
  with hide('warnings'), settings(host_string = env.roledefs['oc'], warn_only = True):
    procs = run('ps -efw | grep -v grep | grep  "nova-api\|nova-conductor\|glance-api\|glance-registry\|cinder-volume\|cinder-scheduler\|nova-scheduler\|nova-consoleauth\|nova-novncproxy\|cinder-api\|nova-cert\|keystone-all"')
    if not re.match('(.|\n)*nova-api.*',procs):
     procs_down.append('nova-api')
  if not re.match('(.|\n)*nova-conductor.*',procs):
    procs_down.append('nova-conductor')
  if not re.match('(.|\n)*glance-api.*',procs):
    procs_down.append('glance-api')
  if not re.match('(.|\n)*glance-registry.*',procs):
    procs_down.append('glance-registry')
  if not re.match('(.|\n)*cinder-volume.*',procs):
    procs_down.append('cinder-volume')
  if not re.match('(.|\n)*cinder-scheduler.*',procs):
    procs_down.append('cinder-scheduler')
  if not re.match('(.|\n)*nova-scheduler.*',procs):
    procs_down.append('nova-scheduler')
  if not re.match('(.|\n)*nova-consoleauth.*',procs):
    procs_down.append('nova-consoleauth')
  if not re.match('(.|\n)*nova-novncproxy.*',procs):
    procs_down.append('nova-novncproxy')
  if not re.match('(.|\n)*cinder-api.*',procs):
    procs_down.append('cinder-api')
  if not re.match('(.|\n)*nova-cert.*',procs):
    procs_down.append('nova-cert')
  if not re.match('(.|\n)*keystone-all.*',procs):
    procs_down.append('keystone-all')

  if len(procs_down) != 0:
    log("Procs down: %s" % ",".join(procs_down))
    return 1
  else:
    return 0

## Fabric finished

